{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import math\n",
    "from pathlib import Path\n",
    "from utils import Tokenizer, Embedding\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from dataset import Seq2SeqDataset\n",
    "from tqdm import tqdm\n",
    "from tqdm import tnrange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "with open(\"../datasets/seq2seq/train.pkl\", \"rb\") as f:\n",
    "    training = pickle.load(f)\n",
    "with open(\"../datasets/seq2seq/valid.pkl\", \"rb\") as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(\"../datasets/seq2seq/embedding.pkl\", \"rb\") as f:\n",
    "    embedding = pickle.load(f)\n",
    "    \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding, embedding_dim, hidden_size,\n",
    "                 n_layers=1, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        emb = nn.Embedding(input_size, embedding_dim)\n",
    "        emb.weight.data.copy_(embedding.vectors)\n",
    "        self.embedding = emb\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.rnn = nn.GRU(300, hidden_size, n_layers, dropout = dropout ,bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout, inplace=False)\n",
    "\n",
    "    def forward(self, src, hidden=None):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # sum bidirectional outputs\n",
    "        hidden = torch.tanh(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "#         print(hidden)\n",
    "        return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding, embedding_dim, enc_hidden_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_size = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.emb_dim = embedding_dim\n",
    "    \n",
    "        emb = nn.Embedding(output_dim, embedding_dim)\n",
    "        emb.weight.data.copy_(embedding.vectors)\n",
    "        self.embedding = emb\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_dim, hid_dim, dropout=0.5)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim , bias=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        #prediction = [batch size, output dim]\n",
    "        return prediction, hidden\n",
    "\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "#         assert encoder.hidden_size == decoder.hidden_size, \\\n",
    "#             \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "#         print(src)\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        hidden = hidden.unsqueeze(0)\n",
    "#         hidden = hidden.view(self.encoder.n_layers, batch_size, -1)\n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(embedding.vocab)\n",
    "OUTPUT_DIM = len(embedding.vocab)\n",
    "ENC_EMB_DIM = 300\n",
    "DEC_EMB_DIM = 300\n",
    "ENC_HID_DIM = 128\n",
    "DEC_HID_DIM = 256\n",
    "N_LAYERS = 1\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, embedding,ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, embedding, ENC_EMB_DIM,ENC_HID_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "# model.load_state_dict(torch.load('./save/seq2seq(no_attention)-model3.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(97513, 300)\n",
       "    (rnn): GRU(300, 128, dropout=0.5, bidirectional=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(97513, 300)\n",
       "    (rnn): GRU(300, 256, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=256, out_features=97513, bias=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def init_weights(m):\n",
    "#     for name, param in m.named_parameters():\n",
    "#         nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "# model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    print_every = 500\n",
    "    print_loss = 0\n",
    "\n",
    "    for i, batch in tqdm(enumerate(iterator)):\n",
    "        src = batch['text'].t().to(device)\n",
    "        trg = batch['summary'].t().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        print_loss += loss.item()\n",
    "        if(i % print_every == 0 and i):\n",
    "            print(\"avg training loss:\", print_loss/print_every)\n",
    "            print_loss = 0\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "#     model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch['text'].t().to(device)\n",
    "            trg = batch['summary'].t().to(device)\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/littleemma/ImageDB/imagedb/lib/python3.6/site-packages/ipykernel_launcher.py:9: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d09a5aa161493c83d8b18b803275d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab0aa4a9a6746d29d14c8a87bc7f06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg training loss: 5.007659989356995\n",
      "avg training loss: 4.97812202835083\n",
      "avg training loss: 4.988719362258911\n",
      "avg training loss: 4.976480197906494\n",
      "avg training loss: 4.98577742099762\n",
      "avg training loss: 4.997849676132202\n",
      "avg training loss: 5.012466372013092\n",
      "avg training loss: 4.982708010673523\n",
      "\n",
      "Epoch: 01 | Time: 27m 27s\n",
      "\tTrain Loss: 4.989 | Train PPL: 146.772\n",
      "\t Val. Loss: 6.423 |  Val. PPL: 616.011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b384644e04a4bccbad24a0062348a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg training loss: 4.92975253200531\n",
      "avg training loss: 4.925217782497406\n",
      "avg training loss: 4.94553226184845\n",
      "avg training loss: 4.920001559257507\n",
      "avg training loss: 4.919572986125946\n",
      "avg training loss: 4.903115026473999\n",
      "avg training loss: 4.9238519268035885\n",
      "avg training loss: 4.934251097202301\n",
      "\n",
      "Epoch: 02 | Time: 27m 24s\n",
      "\tTrain Loss: 4.925 | Train PPL: 137.705\n",
      "\t Val. Loss: 6.399 |  Val. PPL: 601.464\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d04d0808074293ae2d267c92448947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c9996c9cd79d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-3d0016486445>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ImageDB/imagedb/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ImageDB/imagedb/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(self, p, dim, keepdim)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fro\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;34mr\"\"\"See :func: `torch.norm`\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbtrifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ImageDB/imagedb/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out)\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"nuc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fro\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 30\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "train_iterator = torch.utils.data.DataLoader(training, BATCH_SIZE, shuffle=True,collate_fn=valid.collate_fn)\n",
    "valid_iterator = torch.utils.data.DataLoader(valid, BATCH_SIZE, shuffle=True,collate_fn=valid.collate_fn)\n",
    "\n",
    "for epoch in tnrange(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './save/seq2seq(no_attention)-model3.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "def generate_pair(data):\n",
    "    pairs = []\n",
    "    input_length = len(data.data)\n",
    "    for i in range(input_length):\n",
    "        input = data.__getitem__(i)['text']\n",
    "        output = data.__getitem__(i)['summary']\n",
    "        id = data.__getitem__(i)['id']\n",
    "        pairs.append((input, output, id))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, decoder_hidden, last_idx=SOS_token, sentence_idxes=[], sentence_scores=[]):\n",
    "        if(len(sentence_idxes) != len(sentence_scores)):\n",
    "            raise ValueError(\"length of indexes and scores should be the same\")\n",
    "        self.decoder_hidden = decoder_hidden\n",
    "        self.last_idx = last_idx\n",
    "        self.sentence_idxes =  sentence_idxes\n",
    "        self.sentence_scores = sentence_scores\n",
    "\n",
    "    def avgScore(self):\n",
    "        if len(self.sentence_scores) == 0:\n",
    "            raise ValueError(\"Calculate average score of sentence, but got no word\")\n",
    "        # return mean of sentence_score\n",
    "        return sum(self.sentence_scores) / len(self.sentence_scores)\n",
    "\n",
    "    def addTopk(self, topi, topv, decoder_hidden, beam_size, voc):\n",
    "        topv = torch.log(topv)\n",
    "        terminates, sentences = [], []\n",
    "        for i in range(beam_size):\n",
    "            if topi[0][i] == EOS_token:\n",
    "                terminates.append(([voc[idx.item()] for idx in self.sentence_idxes] + ['<EOS>'],\n",
    "                                   self.avgScore())) # tuple(word_list, score_float\n",
    "                continue\n",
    "            idxes = self.sentence_idxes[:] # pass by value\n",
    "            scores = self.sentence_scores[:] # pass by value\n",
    "            idxes.append(topi[0][i])\n",
    "            scores.append(topv[0][i])\n",
    "            sentences.append(Sentence(decoder_hidden, topi[0][i], idxes, scores))\n",
    "        return terminates, sentences\n",
    "\n",
    "    def toWordScore(self, voc):\n",
    "        words = []\n",
    "        for i in range(len(self.sentence_idxes)):\n",
    "            if self.sentence_idxes[i] == EOS_token:\n",
    "                words.append('<EOS>')\n",
    "            else:\n",
    "                words.append(voc[self.sentence_idxes[i].item()])\n",
    "        if self.sentence_idxes[-1] != EOS_token:\n",
    "            words.append('<EOS>')\n",
    "        return (words, self.avgScore())\n",
    "\n",
    "def beam_decode(decoder, decoder_hidden, encoder_outputs, voc, beam_size, max_length=40):\n",
    "    terminal_sentences, prev_top_sentences, next_top_sentences = [], [], []\n",
    "    prev_top_sentences.append(Sentence(decoder_hidden))\n",
    "    for i in range(max_length):\n",
    "        for sentence in prev_top_sentences:\n",
    "            decoder_input = torch.LongTensor([sentence.last_idx])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            decoder_hidden = sentence.decoder_hidden\n",
    "            decoder_output, decoder_hidden, _ = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            topv, topi = decoder_output.topk(beam_size)\n",
    "            term, top = sentence.addTopk(topi, topv, decoder_hidden, beam_size, voc)\n",
    "            terminal_sentences.extend(term)\n",
    "            next_top_sentences.extend(top)\n",
    "\n",
    "        next_top_sentences.sort(key=lambda s: s.avgScore(), reverse=True)\n",
    "        prev_top_sentences = next_top_sentences[:beam_size]\n",
    "        next_top_sentences = []\n",
    "\n",
    "    terminal_sentences += [sentence.toWordScore(voc) for sentence in prev_top_sentences]\n",
    "    terminal_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    n = min(len(terminal_sentences), 15)\n",
    "    return terminal_sentences[:n]\n",
    "\n",
    "def decode(decoder, decoder_hidden, encoder_outputs, voc, max_length=40):\n",
    "\n",
    "    decoder_input = torch.LongTensor([SOS_token])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length) #TODO: or (MAX_LEN+1, MAX_LEN+1)\n",
    "#     print(decoder_hidden.size())\n",
    "\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden)\n",
    "        _, topi = decoder_output.topk(3)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(voc[ni.item()])\n",
    "\n",
    "        decoder_input = torch.LongTensor([ni])\n",
    "        decoder_input = decoder_input.to(device)\n",
    "\n",
    "    return decoded_words, decoder_attentions[:di + 1]\n",
    "\n",
    "\n",
    "def evaluate(encoder, decoder, voc, sentence, beam_size = 1, max_length=40):\n",
    "    indexes_batch = [sentence] #[1, seq_len]\n",
    "    lengths = [len(indexes) for indexes in indexes_batch]\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    input_batch = input_batch.to(device)\n",
    "    batch_size = input_batch.size(1)\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batch, None)\n",
    "\n",
    "    decoder_hidden = encoder_hidden.view(encoder.n_layers, batch_size, -1)\n",
    "    if beam_size == 1:\n",
    "        return decode(decoder, decoder_hidden, encoder_outputs, voc)\n",
    "    else:\n",
    "        return beam_decode(decoder, decoder_hidden, encoder_outputs, voc, beam_size)\n",
    "\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, tokenizer, voc, pairs, reverse, beam_size, n=10):\n",
    "    for _ in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print(\"=============================================================\")\n",
    "        if reverse:\n",
    "            print('>', \" \".join(reversed(pair[0].split())))\n",
    "        else:\n",
    "            print('>', tokenizer.decode(pair[0]))\n",
    "            print('=', tokenizer.decode(pair[1]))\n",
    "        if beam_size == 1:\n",
    "            output_words, _ = evaluate(encoder, decoder, voc, pair[0], beam_size)\n",
    "            output_sentence = ' '.join(output_words)\n",
    "            print('<', output_sentence)\n",
    "        else:\n",
    "            output_words_list = evaluate(encoder, decoder, voc, pair[0], beam_size)\n",
    "            for output_words, score in output_words_list:\n",
    "                output_sentence = ' '.join(output_words)\n",
    "                print(\"{:.3f} < {}\".format(score, output_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================\n",
      "> the case relates to a visit last year from a lawyer from the international criminal court ( icc ) who was accused of passing information to mr gaddafi . <unk> mr gaddafi has also been indicted for war crimes during the 2011 uprising . <unk> both libya and the icc claim jurisdiction for that trial . <unk> when asked whether he was in good health , mr gaddafi said that he was and gave a thumbs - up sign , the bbc 's foreign editor , john simpson , reports from <unk> . <unk> mr gaddafi faces charges of complicity in exchanging information , obtaining documents that threaten national security and insulting the national flag . <unk> evidence was briefly presented at the hearing , including a pen with a camera in it and a watch , which the prosecution alleges were used in passing illicit information . <unk> representatives from local and international human rights organisations were also present at the hearing . <unk> the trial has been postponed until 19 september . <unk> in june last year , icc lawyer melinda taylor and three other icc staff were arrested and held for the three weeks after visiting mr gaddafi . <unk> ms taylor was accused of clandestinely passing mr gaddafi a coded letter from a fugitive former aide , mohammed ismail . <unk> the four were subsequently released to the hague and are not expected to return to libya to face charges . <unk> the icc said last year that it would investigate the allegations . <unk> mr gaddafi has been held in <unk> since a brigade from the town captured him in november 2011 . <unk> his british lawyer john jones described the detention to the bbc as \" libya 's guantanamo bay \" and said he\n",
      "= <s> the son of deposed libyan leader col muammar gaddafi , saif al - islam , has briefly appeared in court to face criminal charges and has been appointed two local lawyers . <unk> </s>\n",
      "torch.Size([1, 1, 256])\n",
      "< a us - of - has has been has been a in the the of of the , of the . <unk> <EOS>\n",
      "=============================================================\n",
      "> the trust had been rapped over the way it investigated patient deaths with calls for katrina percy to resign . <unk> an investigation commissioned by nhs england found only 272 of the 722 deaths in the trust over the previous four years were dealt with properly . <unk> but ms percy will continue in her role , interim chair tim smart has said . <unk> in april , inspectors said the trust was \" continuing to put patients at risk \" . <unk> then in june , the trust accepted responsibility for the death of 18-year - old connor sparrowhawk , who drowned in a bath at one of its facilities - slade house in oxford . <unk> it admitted it \" caused \" the death of connor - who had suffered an epileptic seizure before he died in july 2013 - and offered his family <unk> compensation . <unk> following a six - week comprehensive review however , mr smart said it was clear the executive team had been \" too stretched to guarantee high quality services \" . <unk> he recommended the trust should \" transform the way in which it delivers services , and makes changes to the structure and strength of its leadership team \" . <unk> \" [ i am ] satisfied that whilst the board should have acted in a more united way , i have found no evidence of negligence or incompetence of any individual board member , \" he said . <unk> regarding ms percy , he said she had been \" too operationally focused \" and this would be shifted to oversee the \" delivery of the future strategy of the trust , which i believe needs to be accelerated \" . <unk> southern health nhs foundation trust covers hampshire ,\n",
      "= <s> the chief executive of the much - criticised southern health nhs foundation trust will keep her job , it has been announced . <unk> </s>\n",
      "torch.Size([1, 1, 256])\n",
      "< a of of a has been a to the to the a of a \" - \" \" , the the has said . <unk> <EOS>\n",
      "=============================================================\n",
      "> media playback is not supported on this device <unk> bolt , 29 , clocked 19.89 seconds in his first appearance since pulling out of the jamaican trials . <unk> he was picked for the olympics after getting a medical exemption , leading to comments from gatlin and others . <unk> \" for me i felt it was a joke , i felt it was a disrespect they think i 'd back out of a trials , \" said bolt . <unk> \" i 've proven myself year on year that i 'm the greatest . i laughed when i heard it , i was disappointed , especially in justin gatlin . \" <unk> media playback is not supported on this device <unk> bolt won gold in both the 100 m and 200 m at the london and beijing olympics , and will defend both titles in rio next month . <unk> gatlin , considered his main 100 m rival , was among those who claimed the jamaican - who has struggled with a hamstring injury - got preferential treatment he would not receive were he american . <unk> \" he 's injured , gets a medical pass , that 's what his country does . our country does n't do that , \" gatlin , who has twice served doping bans , was quoted as saying in the american press . <unk> at the olympic stadium on friday night , bolt finished 0.15 seconds ahead of panama 's alonso edward in second , with britain 's commonwealth silver medallist adam <unk> third . <unk> the fastest 200 m time of the year is 19.74 by united states sprinter <unk> merritt , while bolt 's word record - set in 2009 - is 19.19 . <unk> media playback is not supported\n",
      "= <s> usain bolt accused sprint rival justin gatlin of \" disrespect \" after the six - time olympic champion won the 200 m at the anniversary games in london . <unk> </s>\n",
      "torch.Size([1, 1, 256])\n",
      "< former england captain tom <unk> has been named as the of the \" after \" the \" . <unk> <EOS>\n",
      "=============================================================\n",
      "> the victim , who is 34 , was attacked in <unk> link in the north of the city at about 01:15 bst on sunday . <unk> he was said to be in a stable condition in hospital on sunday . <unk> the accused , both age 26 , are due to appear before belfast magistrates ' court later on monday . <unk> </s>\n",
      "= <s> two men have been charged with attempted murder after a man was hit on the head with a hatchet in belfast . <unk> </s>\n",
      "torch.Size([1, 1, 256])\n",
      "< a man - old man has been arrested on suspicion of murder after a man was found in a . <unk> <EOS>\n",
      "=============================================================\n",
      "> <unk> <unk> , 34 , is accused of raping the 15-year - old on 29 november 2013 , and sexually assaulting and falsely imprisoning the older girl on 24 january this year . <unk> all of the alleged offences happened in bolton , greater manchester police said . <unk> <unk> , of great lever , bolton , appeared before magistrates in the town and was remanded in custody . <unk> he is scheduled to appear at bolton crown court on 17 july . <unk> </s>\n",
      "= <s> a man has been charged with the sexual abuse of two girls aged 15 and 16 in greater manchester . <unk> </s>\n",
      "torch.Size([1, 1, 256])\n",
      "< a man has been charged with murder after a man was found in a car in a . <unk> <EOS>\n",
      "=============================================================\n",
      "> it is billed as one of the biggest events of its kind in europe and <unk> last year attracted more than 30,000 visitors . <unk> this year , organisers invited people to submit photos for the people 's gallery based on the theme chaos and calm . <unk> the best 76 images are to be exhibited at the show - here is a selection . <unk> </s>\n",
      "= <s> amateur and professional photographers from across the uk were invited to submit pictures for the forthcoming photography show at the nec near birmingham . <unk> </s>\n",
      "torch.Size([1, 1, 256])\n",
      "< the uk 's to be the in the the of of the , the . <unk> <EOS>\n",
      "=============================================================\n",
      "> the man was walking at the junction between galloway street and glasgow street at 21:30 when he was struck by a green vauxhall adam . <unk> he was taken to hospital in dumfries before being transferred to edinburgh western infirmary . <unk> staff have described his condition as serious . <unk> police have asked anyone who saw the incident to contact them . <unk> </s>\n",
      "= <s> a 19-year - old man is being treated for serious injuries after being hit by a car in dumfries town centre on christmas eve . <unk> </s>\n",
      "torch.Size([1, 1, 256])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< a man - old man has been arrested in connection with the death of a man in a . <unk> <EOS>\n",
      "=============================================================\n",
      "> the ex - england hooker , 57 , won the competition with devon in 2004 as a player and as coach in 2007 , and played in cornwall 's triumphant 1991 campaign . <unk> dawe has been cornwall head coach since 2013 , leading them to victories in the 2015 and 2016 twickenham finals . <unk> \" it 's a nice local game for me , but i 'm travelling as a <unk> , \" he said . <unk> \" i 'm 100 % behind cornwall , there 's no emotion at all . \" <unk> the game will be played at ivybridge on saturday , with cornwall favourites as they continue their pursuit of a hat - trick of county championship titles against a devon side who were only promoted to division one because of a restructuring of the competition . <unk> devon head coach dan parkes played under dawe at plymouth albion , and acknowledges his side are underdogs . <unk> \" it 's going to be hard , the boys are going to be up against it , \" he told bbc sport . <unk> \" for graham the pressure is on , but i 'm sure they 'll get to their third final . <unk> \" the better we can do , the more likely we can attract players next year . players play for cornwall because they know they have a great chance of reaching twickenham . \" <unk> </s>\n",
      "= <s> graham dawe says he has \" no emotion \" ahead of his cornwall side 's county championship game at devon , who he previously played for and coached . <unk> </s>\n",
      "torch.Size([1, 1, 256])\n",
      "< england 's will not compete at the world 's world cup in the after a a - the <unk> <EOS>\n",
      "=============================================================\n",
      "> additional funding of £ 5 m has been allocated to the budget of the gosport independent panel for 2017/18 after the timescale of the inquiry was extended . <unk> relatives had called for a public inquiry which they believe would have been quicker and cheaper . <unk> launched in 2014 , the investigation was originally due to end in december 2017 . <unk> a review of the deaths at gosport war memorial hospital between 1988 and 2000 found an \" almost routine use of opiates had almost certainly \" shortened the lives of some patients . <unk> police investigated the deaths of 92 people but brought no prosecutions . <unk> the end date of the review was pushed back in november 2016 with the work likely to end in spring 2018 , the government previously said . <unk> the gosport independent panel is headed by the former bishop of liverpool james jones , who led the hillsborough inquiry . <unk> bridget reeves , whose grandmother elsie devine died at the hospital , is among the relatives who criticised the decision for an independent panel . <unk> she said : \" the frustration is that when we started we wanted a public inquiry but we were told very clearly that the cost would be far too great . \" <unk> department of health allocated budgets to the gosport independent panel : <unk> the department of health said : \" budgets may be reviewed by the new government following the election on 8 june . \" <unk> </s>\n",
      "= <s> the inquiry into the deaths of dozens of elderly patients at a hampshire hospital will cost more than £ 13 m , the department of health has revealed . <unk> </s>\n",
      "torch.Size([1, 1, 256])\n",
      "< a of of a been found in a a in in a in a in the . <unk> <EOS>\n",
      "=============================================================\n",
      "> the incident happened in silverstream crescent in the north of the city on friday morning . <unk> it was reported to the <unk> just after 08:00 bst . the police are treating the incident as criminal damage . <unk> the victim lives alone and was very distressed by the incident , a relative told the bbc . <unk> </s>\n",
      "= <s> police are investigating after a disabled woman 's belfast home was attacked with paint . <unk> </s>\n",
      "torch.Size([1, 1, 256])\n",
      "< a man - old - old boy has been arrested in hospital after being hit by a car in a . <unk> <EOS>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(embedding.vocab, lower=False)\n",
    "pairs = generate_pair(valid)\n",
    "evaluateRandomly(enc, dec, tokenizer, embedding.vocab, pairs, False, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './save/seq2seq(no_attention)-model-final.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4f95f93961ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeq2Seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./save/seq2seq(no_attention)-model-final.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ImageDB/imagedb/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './save/seq2seq(no_attention)-model-final.pt'"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq(enc, dec, device)\n",
    "model.load_state_dict(torch.load('./save/seq2seq(no_attention)-model-final.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/littleemma/ImageDB/imagedb/lib/python3.6/site-packages/ipykernel_launcher.py:14: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b88db04bbf4c69b7bd08429f1d3bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> the building was to be pulled down as part of a redevelopment of the lawn complex in lincoln . <unk> bosses at woodside wildlife park have now stepped in to save it and will move it to their site near <unk> . <unk> it will be used as an attraction housing exotic animals and coral reef aquariums . <unk> the conservatory is named after the lincolnshire botanist who travelled with captain james cook on his first voyage to the south pacific in 1768 . <unk> the grade ii listed lawn complex was sold by city of lincoln council last year to the stokes coffee company which plans to open a cafe and museum on the site . <unk> neil <unk> , director of the wildlife park , said he wanted to save the building , which housed exotic plants and koi carp and was popular with generations of families . <unk> \" i , like a lot of people , spent my younger days coming here and bringing my children here , \" he said . <unk> \" when i heard that it was being demolished and closing down , i thought we were probably in as good a place as anybody to save it . \" <unk> once rebuilt , the conservatory will be used for education and conservation projects , while retaining its heritage as a journey of discovery , mr <unk> said . <unk> he said the plan to house crocodiles , red pandas and exotic plants inside it would \" highlight changes in our planet \" since sir joseph banks ' voyage of discovery . <unk> the council said it had donated a parcel of land at the lawn to the sir joseph banks society to build a new conservatory . <unk> </s>\n",
      "< a new of has been to a a of a a in the the of of the . <unk>\n",
      "> a national college of teaching and leadership panel found marc richardson , 34 , guilty of unacceptable professional conduct . <unk> the panel heard cctv footage showed the pair emerging from beneath a stage at tottington high school , bury , where he was head of drama . <unk> the married teacher had been previously warned about his behaviour . <unk> in 2012 , he sent \" inappropriate communications \" to another student , whom he said was \" too attractive to teach \" and asked her to keep quiet . he was given a final written warning by the school in january 2013 . <unk> richardson , who did not attend the hearing , admitted being alone with a student in a room under the stage outside teaching hours in 2014 . <unk> he resigned , was arrested and released without charge . <unk> the panel heard he sent messages to the girl , engaging in sexual conversations . <unk> her mother said she had \" niggling feelings \" about the teacher when her daughter asked about the contraceptive pill . <unk> another witness said her friend told her she had \" done everything \" with her boyfriend , who she admitted was richardson . <unk> mark tweedle , panel chairman , said : \" the panel considered it is plain that sexual conversations took place between pupil b and mr richardson , and they contained explicit references to sexual acts between them . \" <unk> he said richardson 's conduct fell \" significantly short of the standards expected of the profession . \" <unk> a spokesman for greater manchester police said of the arrest in 2014 : \" a 32-year - old man was arrested on suspicion of sexual activity with a child and the sexual grooming of\n",
      "< a man has been jailed for life - threatening \" and a \" a \" of a \" . <unk>\n",
      "> <unk> , 19 , and ball , 20 , represent manager mark warburton 's eighth and ninth signings of the summer window . <unk> both players are a product of the tottenham youth system . <unk> ball spent the second half of last season on loan at cambridge united , making 11 appearances , whilst <unk> also appeared 11 times for luton town . <unk> ball has represented england at under-19 and under-20 level , having previously represented northern ireland at younger age groups . <unk> <unk> has also played for england at youth level , representing his country at the under-18 level . <unk> warburton will have both players available this sunday as rangers continue their championship campaign away to alloa . <unk> </s>\n",
      "< <unk> <unk> have signed <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def predict_out(list_dict, file_path):\n",
    "    with open(file_path , 'w') as outfile:\n",
    "        for entry in list_dict:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "def predict(encoder, decoder, voc, pairs):\n",
    "    n = len(pairs)\n",
    "    out = []\n",
    "    show_per = 4000\n",
    "    for i in tnrange(n):\n",
    "        predict = {}\n",
    "        pair = pairs[i]\n",
    "        output_words, attention = evaluate(encoder, decoder, voc, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        if(i % show_per == 0):\n",
    "            print('>', tokenizer.decode(pair[0]))\n",
    "            print('<', output_sentence[:-6])\n",
    "#             show_attention(tokenizer.decode(pair[0]), output_sentence, attention)\n",
    "        predict['id'] = pair[2]\n",
    "        predict['predict'] = output_sentence[:-6]\n",
    "        out.append(predict)\n",
    "    return out\n",
    "\n",
    "pairs = generate_pair(valid)\n",
    "out = predict(model.encoder, model.decoder, embedding.vocab, pairs)\n",
    "predict_out(out, \"../data/seq2seq_output.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imagedb",
   "language": "python",
   "name": "imagedb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
